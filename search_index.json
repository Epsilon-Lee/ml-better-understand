[
["index.html", "Epsilon’s Machine Learning Notes Prelogue", " Epsilon’s Machine Learning Notes Epsilon Guanlin Li Aug. 31 2019 Prelogue The inspiration of doing this notes is through the encouraging of a quote that I love most: “But you go to a great school, not for knowledge so much as for arts and habits; for the habit of attention, for the art of expression, […] for the art of entering quickly into another person’s thoughts, […] for taste, for discrimination, for mental courage and mental soberness.” — William Johnson Corg. Knowledge is for distilling, critisizing and sharing. That is the taste of this notes. Specifically, I am also inspired by several researchers and their works in this field of machine learning: Zachary C. Lipton and Jacob Steinhardt’s Troubling Trends in Machine Learning Scholarship, based on which I try to distinguish the real inspiring and will-be-test-of-time works. Ferenc Huszár’s blog inFERENCe, the style of simplicity and thought-provoking. Notes by Karl Stratos, succint and in-depth reformulation. Francis Bach’s blog on statistics and optimization, real master piece! Cosma Rohilla Shalizi’s notebook, very Retro. "],
["computational-and-statistical-learning-theory.html", "Chapter 1 Computational and Statistical Learning Theory", " Chapter 1 Computational and Statistical Learning Theory This chapter covers the following topics: PAC and PAC Bayes framework. "],
["pac-framework.html", "1.1 PAC Framework", " 1.1 PAC Framework The PAC learning framework is the first computational learning model proposed by Valiant in the 1980s. "],
["topics-in-deep-learning.html", "Chapter 2 Topics in Deep Learning", " Chapter 2 Topics in Deep Learning This chapter is dedicated to several important and interesting topics in the (recent) development of deep learning with theoretic considerations. I will highlight the theory-oriented part of the story for every topics. The mainly covered topics are listed below: Generalization in Deep Learning; Landscape and visualization; Optimization-related insights; initialization; learning schedule; curriculum learning; "],
["interpretability.html", "Chapter 3 Interpretability", " Chapter 3 Interpretability Interpretability is recently received huge attention in the AI community, especially for the data-driven machine learning paradigm. “Papers provide diverse and sometimes non-overlapping motivations for interpretability.” (Lipton 2018). "],
["attribution.html", "3.1 Attribution", " 3.1 Attribution Interpretability is a means to build trustworthy machine learning system which can generate ‘rationales’ to explain why it make a decision. Today (Sep. 19), we are going to dig into a recent NeurIPS paper Robust Attribution Regularization which introduces the concept of robust attribution. Their work is built on the Integrated Gradient (IG) methods by proposing training objectives in classic robust optimization to achieve robust IG. 3.1.1 Saliency with Guarantees Discovering Conditionally Salient Features with Statistical Guarantees, ICML 2019. Interpreting Black-Box Models via Hypothesis Testing, arXiv Apr. 2019. Statistical Consistent Saliency Estimation, ICLR 2020. "],
["knowledge-tracing.html", "3.2 Knowledge Tracing", " 3.2 Knowledge Tracing This direction of interpretability research reflects many researchers’ interests on the possible knowledge in the training data that can be learned by the model who applies those generalizable knowledge to perform well on unseen instances. This is a data-centered view of machine learning. It can shed light on agnostic machine learning from where any form of hypothesis class can be inspired. To me, the first data-centric shift of interpretability research starts from Koh and Liang’s Understanding Black-box Predictions via Influence Functions (Koh and Liang 2017). They renew a classic technique in robust statistics called influence function to analyze the training effect of single data point removel: how does that data point influence the prediction behavior of the model \\(\\mathcal{M}\\) on unseen instance \\(\\mathbb{x}\\). To think more about the research focus of Koh and Liang (2017), they are trying to efficiently solve the estimation or optimization problem: \\[ \\arg\\min_{\\theta} \\frac{1}{N-1} \\sum_{\\mathbb{z} \\in \\mathcal{D}^{tr}_{\\setminus i}} \\mathcal{L}(z; \\theta) \\] where \\(\\mathbb{z}=(\\mathbb{x}, y)\\) is a training instance, \\(\\mathcal{D}^{tr}\\) is the training set with \\(N\\) instances in total. \\(\\mathcal{D}^{tr}_{\\setminus i}\\) means remove the \\(i\\)-th instance from the training set. The above estimation problem has the same form of the original estimation problem where the \\(i\\)-the instance is not removed. This seems familar to tranditional statistical learning researchers since this is just one-round of leave-one-out cross-validation. 3.2.1 Literature Survey This section gives a literature survey based on papers collected in this file. "],
["compositionality.html", "3.3 Compositionality", " 3.3 Compositionality Compositionality is a concept from linguistic philosophy, it means in a symbol system, e.g. the English language, the meaning of a symbol sequence can be computed through the composition (a meaning composition function) of its sub-sequences. 3.3.1 Related Works in ICLR 2020 This part is a summary of the recent published ICLR20 anonymous submissions. You can find it in the paper buffer section for compositionality. "],
["understanding-internal-representation.html", "3.4 Understanding internal representation", " 3.4 Understanding internal representation Neural networks learn continuous hidden representations as a byproduct of the main supervised or unsupervised task. There are many works try to shed light on the question: what interpretable information is learned and entailed in these hidden representations. This article is try to group them in a systematic way and try to seek for frontiers in this pursuit. Representation probe is one of the most frequently used technique for representation explanation, see Table SM1 in (Belinkov and Glass 2019) for a enumerative review of recent efforts on this very direction. However, a recent EMNLP 2019 best paper runner up (Hewitt and Liang 2019) argue that the capacity of the probe should be considered while using the constructed probing task. Let us discuss their benefit and criticism at the same time below. "],
["robustness-adversary-and-causality.html", "Chapter 4 Robustness, Adversary and Causality", " Chapter 4 Robustness, Adversary and Causality I think robust AI or trustful AI is the future of the AI systems that will be deployed to interact with human beings and facilitate their daily lives because of the guarantee of their worst-case behavior. However, current machine learning models are purely data-driven with less human prior knowledge and no guarantee or failure protection against unseen adversaries. The reason for such crispness might reside in the model’s ignorance of causality underlying the observations or resulted from current input representation with high dimensionality that causes unknown mystery of bug in that geometric space. To clarify this intertwined topic, in this chapter, I would like to search for hidden connections between robustness and causality or high dimensional statistics. The discussions mainly focus on model family of recent advanced deep neural networks. However, robustness and adversary are more historical concepts. The initial coverage of topics includes: Adversary and adversarial training; Adversarial training and task performance; Certified (or verified, provable) robustness; Applications in natural language processing. "],
["applications.html", "4.1 Applications", " 4.1 Applications This section will introduce several case studies that apply robustness techniques in real world applications, such as image or text classification. The concept of verified or certified robustness is a principled concept in the study of robustness in learning. It is a mathematical concept though can be smartly achieved in realistic applications. Let’s see how researchers resolve this problem of theory-reality mismatch. 4.1.1 Certified Robustness in Text Classification Two papers (Jia et al. 2019) (Huang et al. 2019) accepted in EMNLP 2019 happen to be commonly focusing on certified robustness of text classifiers, and they even use the same technique for computing the bounded loss in the worst case via the co-called Interval Bound Propagation (IBP), a technique first appeared in Dvijotham et al. (2018) with applications to image classification. I list their name in the following: Certified Robustness to Adversarial Word Substitutions, Robin Jia et al at Stanford University. Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation, Po-Sen Huang et al at DeepMind. 4.1.1.1 Adversaries in Text Classification The task of text classification is defined as standard classification setting, with \\(\\mathcal{Y}\\) as the label set, \\(\\mathcal{X}\\) as the input domain. Specifically, each \\(\\mathbb{x} = (\\mathbb{x}_1, \\dots, \\mathbb{x}_m) \\in \\mathcal{X}\\) is a discourse (sentence or paragraph) containing a sequence of discrete symbols within a vocabulary, that is, \\(\\mathbb{x}_i \\in \\mathcal{V}\\). 4.1.2 Adversarial Examples for Natural Language This part summarizes several works for generating (natural) adversarial examples for natural langauge inputs. HotFlip (Ebrahimi et al. 2017): Paraphrase-based advesarials (Ribeiro, Singh, and Guestrin 2018): 4.1.2.0.1 How to group adversarial papers in NLP? Generating Natural Language Adversarial Examples, arXiv Apr. 2018. Robust Neural Machine Translation with Doubly Adversarial Inputs, ACL 2019. Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification, SysML 2019. GenAttack: Practical Black-box Attacks with Gradient-Free Optimization, arXiv May 2018. "],
["out-of-distribution-learning.html", "Chapter 5 Out-of-Distribution Learning", " Chapter 5 Out-of-Distribution Learning Jan. 10 2022 This part is mainly about machine learning under Out-of-Distribution (OOD) setting and OOD detection for trustworthy machine learning. The papers summarized here are collected by this file from my github repository. I would like to first cover several crucial works about OOD detection and then how to generalize under OOD setting. "],
["references.html", "References", " References "]
]
