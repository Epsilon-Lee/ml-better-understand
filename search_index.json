[
["index.html", "Epsilon’s Machine Learning Notes Prelogue", " Epsilon’s Machine Learning Notes Epsilon Guanlin Li Aug. 31 2019 Prelogue The inspiration of doing this notes is through the encouraging of a quote that I love most: “But you go to a great school, not for knowledge so much as for arts and habits; for the habit of attention, for the art of expression, […] for the art of entering quickly into another person’s thoughts, […] for taste, for discrimination, for mental courage and mental soberness.” — William Johnson Corg. Knowledge is for distilling, critisizing and sharing. That is the taste of this notes. Specifically, I am also inspired by several researchers and their works in this field of machine learning: Zachary C. Lipton and Jacob Steinhardt’s Troubling Trends in Machine Learning Scholarship, based on which I try to distinguish the real inspiring and will-be-test-of-time works. Ferenc Huszár’s blog inFERENCe, the style of simplicity and thought-provoking. Notes by Karl Stratos, succint and in-depth reformulation. Francis Bach’s blog on statistics and optimization, real master piece! Cosma Rohilla Shalizi’s notebook, very Retro. "],
["computational-and-statistical-learning-theory.html", "Chapter 1 Computational and Statistical Learning Theory", " Chapter 1 Computational and Statistical Learning Theory This chapter covers the following topics: PAC and PAC Bayes framework. "],
["pac-framework.html", "1.1 PAC Framework", " 1.1 PAC Framework The PAC learning framework is the first computational learning model proposed by Valiant in the 1980s. "],
["topics-in-deep-learning.html", "Chapter 2 Topics in Deep Learning", " Chapter 2 Topics in Deep Learning This chapter is dedicated to several important and interesting topics in the (recent) development of deep learning with theoretic considerations. I will highlight the theory-oriented part of the story for every topics. The mainly covered topics are listed below: Generalization in Deep Learning; Landscape and visualization; Optimization-related insights; initialization; learning schedule; curriculum learning; "],
["interpretability.html", "Chapter 3 Interpretability", " Chapter 3 Interpretability Interpretability is recently received huge attention in the AI community, especially for the data-driven machine learning paradigm. “Papers provide diverse and sometimes non-overlapping motivations for interpretability.” (Lipton 2018). "],
["attribution.html", "3.1 Attribution", " 3.1 Attribution Interpretability is a means to build trustworthy machine learning system which can generate ‘rationales’ to explain why it make a decision. Today (Sep. 19), we are going to dig into a recent NeurIPS paper Robust Attribution Regularization which introduces the concept of robust attribution. Their work is built on the Integrated Gradient (IG) methods by proposing training objectives in classic robust optimization to achieve robust IG. 3.1.1 Saliency with Guarantees Discovering Conditionally Salient Features with Statistical Guarantees, ICML 2019. Interpreting Black-Box Models via Hypothesis Testing, arXiv Apr. 2019. Statistical Consistent Saliency Estimation, ICLR 2020. "],
["knowledge-tracing.html", "3.2 Knowledge Tracing", " 3.2 Knowledge Tracing This direction of interpretability research reflects many researchers’ interests on the possible knowledge in the training data that can be learned by the model who applies those generalizable knowledge to perform well on unseen instances. This is a data-centered view of machine learning. It can shed light on agnostic machine learning from where any form of hypothesis class can be inspired. To me, the first data-centric shift of interpretability research starts from Koh and Liang’s Understanding Black-box Predictions via Influence Functions (Koh and Liang 2017). They renew a classic technique in robust statistics called influence function to analyze the training effect of single data point removel: how does that data point influence the prediction behavior of the model \\(\\mathcal{M}\\) on unseen instance \\(\\mathbb{x}\\). To think more about the research focus of Koh and Liang (2017), they are trying to efficiently solve the estimation or optimization problem: \\[ \\arg\\min_{\\theta} \\frac{1}{N-1} \\sum_{\\mathbb{z} \\in \\mathcal{D}^{tr}_{\\setminus i}} \\mathcal{L}(z; \\theta) \\] where \\(\\mathbb{z}=(\\mathbb{x}, y)\\) is a training instance, \\(\\mathcal{D}^{tr}\\) is the training set with \\(N\\) instances in total. \\(\\mathcal{D}^{tr}_{\\setminus i}\\) means remove the \\(i\\)-th instance from the training set. The above estimation problem has the same form of the original estimation problem where the \\(i\\)-the instance is not removed. This seems familar to tranditional statistical learning researchers since this is just one-round of leave-one-out cross-validation. "],
["compositionality.html", "3.3 Compositionality", " 3.3 Compositionality Compositionality is a concept from linguistic philosophy, it means in a symbol system, e.g. the English language, the meaning of a symbol sequence can be computed through the composition (a meaning composition function) of its sub-sequences. 3.3.1 Related Works in ICLR 2020 This part is a summary of the recent published ICLR20 anonymous submissions. You can find it in the paper buffer section for compositionality. "],
["robustness-adversary-and-causality.html", "Chapter 4 Robustness, Adversary and Causality", " Chapter 4 Robustness, Adversary and Causality I think robust AI or trustful AI is the future of the AI systems that will be deployed to interact with human beings and facilitate their daily lives because of the guarantee of their worst-case behavior. However, current machine learning models are purely data-driven with less human prior knowledge and no guarantee or failure protection against unseen adversaries. The reason for such crispness might reside in the model’s ignorance of causality underlying the observations or resulted from current input representation with high dimensionality that causes unknown mystery of bug in that geometric space. To clarify this intertwined topic, in this chapter, I would like to search for hidden connections between robustness and causality or high dimensional statistics. The discussions mainly focus on model family of recent advanced deep neural networks. However, robustness and adversary are more historical concepts. The initial coverage of topics includes: Adversary and adversarial training; Adversarial training and task performance; Certified (or verified, provable) robustness; Applications in natural language processing. "],
["applications.html", "4.1 Applications", " 4.1 Applications This section will introduce several case studies that apply robustness techniques in real world applications, such as image or text classification. The concept of verified or certified robustness is a principled concept in the study of robustness in learning. It is a mathematical concept though can be smartly achieved in realistic applications. Let’s see how researchers resolve this problem of theory-reality mismatch. 4.1.1 Certified Robustness in Text Classification Two papers (Jia et al. 2019) (Huang et al. 2019) accepted in EMNLP 2019 happen to be commonly focusing on certified robustness of text classifiers, and they even use the same technique for computing the bounded loss in the worst case via the co-called Interval Bound Propagation (IBP), a technique first appeared in Dvijotham et al. (2018) with applications to image classification. I list their name in the following: Certified Robustness to Adversarial Word Substitutions, Robin Jia et al at Stanford University. Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation, Po-Sen Huang et al at DeepMind. 4.1.1.1 Adversaries in Text Classification The task of text classification is defined as standard classification setting, with \\(\\mathcal{Y}\\) as the label set, \\(\\mathcal{X}\\) as the input domain. Specifically, each \\(\\mathbb{x} = (\\mathbb{x}_1, \\dots, \\mathbb{x}_m) \\in \\mathcal{X}\\) is a discourse (sentence or paragraph) containing a sequence of discrete symbols within a vocabulary, that is, \\(\\mathbb{x}_i \\in \\mathcal{V}\\). 4.1.2 Adversarial Examples for Natural Language This part summarizes several works for generating (natural) adversarial examples for natural langauge inputs. HotFlip (Ebrahimi et al. 2017): Paraphrase-based advesarials (Ribeiro, Singh, and Guestrin 2018): 4.1.2.0.1 How to group adversarial papers in NLP? Generating Natural Language Adversarial Examples, arXiv Apr. 2018. Robust Neural Machine Translation with Doubly Adversarial Inputs, ACL 2019. Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification, SysML 2019. GenAttack: Practical Black-box Attacks with Gradient-Free Optimization, arXiv May 2018. "],
["paper-buffer.html", "Chapter 5 Paper buffer", " Chapter 5 Paper buffer Here we categorize recent papers into several promising topics. "],
["interpretability-1.html", "5.1 Interpretability", " 5.1 Interpretability Explaining Image Classifier by Counterfactual Generation, ICLR 2019. Distribution-Guided Local Explanation for Black-box Classifiers, ICLR 2020. Evaluation and Methods for Explanation through Robustness Analysis, ICLR 2020. Interpretable Network Structure for Modeling Contextual Dependency, ICLR 2020. Certifiably Robust Interpretation for Deep Learning, ICLR 2020. Statistically Consistent Saliency Estimation, ICLR 2020. "],
["generalization-and-meta-learning.html", "5.2 Generalization and meta-learning", " 5.2 Generalization and meta-learning Truth or Backpropaganda? An empirical investigation of deep learning theory, ICLR 2020. Towards Neural Networks that Provably Know When They Don’t Know, ICLR 2020. Calibration, Entropy Rates, and Memory in Language Models, ICLR 2020. Disentangling Trainability and Generalization in Deep Learning, ICLR 2020. The Intriguing Role of Module Criticality in The Generalization of Deep Networks, ICLR 2020. Test-time Training For Out-of-Distribution Generalization, ICLR 2020. Entropy Penalty: Towards Generalization Beyond The IID Assumption, ICLR 2020. On Model Stability as a Function of Random Seed, EMNLP 2019. 5.2.1 Meta-learning Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML, arXiv Sep. 2019. A Meta-Transfer Objective For Learning to Disentangle Causal Mechanisms, ICLR 2020, by Yoshua Bengio. Towards Understanding Generalization in Gradient-Based Meta-Learning, ICLR 2020. 5.2.2 Transfer/Multitask learning The Visual Task Adaptation Benchmark, arXiv Oct. 2019. Gumbel-Matrix Routing for Flexible Multitask Learning, ICLR 2020. The Role of Embedding Complexity in Domain-invariant Representations, arXiv Oct. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, arXiv Oct. 2019. 5.2.3 Memorization Overparameterized Neural Networks Can Implement Associative Memory, arXiv Sep. 2019. 5.2.4 Optimization On Empirical Comparisons of Optimizers for Deep Learning, ICLR 2020. "],
["curriculum-learning.html", "5.3 Curriculum learning", " 5.3 Curriculum learning Dynamic Instance Hardness, ICLR 2020. Curriculum Loss: Robust Learning and Generalization Against Labrl Corruption, ICLR 2020. All Neural Networks Are Created Equally, ICLR 2020. Rethinking Curriculum Learning with Incremental Labels and Adaptive Compensation, ICLR 2020. "],
["understanding-intrinsic-representation.html", "5.4 Understanding intrinsic representation", " 5.4 Understanding intrinsic representation Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel, EMNLP 2019. Towards Interpreting Deep Neural Networks via Understanding Layer Behaviors, ICLR 2020. Understanding and Improving Information Transfer in Multi-Task Learning, ICLR 2020. Attention Interpretability Across NLP Tasks, ICLR 2020. Through the Lens of Neural Network: Analyzing Neural QA Models via Quantized Latent Representation, ICLR 2020. Interpreting CNN Prediction Through Layer-wise Selected Discernible Neurons, ICLR 2020. 5.4.1 Representation learning: principles and methods Regulary Varying Representations for Sentence Embedding, ICLR 2020. dilation invariance property to handle heavy-tail phenomenon; to do data augmentation "],
["data-augmentation.html", "5.5 Data augmentation", " 5.5 Data augmentation Revisiting Self-Training for Neural Sequence Generation, ICLR 2020. Rethinking Data Augmentation: Self-supervision and Self-distillation, ICLR 2020. Unsupervised Data Augmentation for Consistency Training, ICLR 2020. Meta Dropout: Learning to Perturb Latent Features For Generalization, ICLR 2020. Implicit Semantic Data Augmentation for Deep Networks, NeurIPS 2019. When Covariate-Shifted Data Augmentation Increases Test Error And How To Fix It, ICLR 2020. A Critical Analysis of Self-supervision, or What We Can Learn From A Single Image, ICLR 2020. Learning Data Manipulation for Augmentation and Weighting, NeurIPS 2019. "],
["robustness.html", "5.6 Robustness", " 5.6 Robustness 5.6.1 Certified robustness Robustness Verification for Transformers, ICLR 2020. Towards Certified Defense for Unrestricted Adversarial Attacks, ICLR 2020. 5.6.2 Adversarial examples Fooling Pre-trained Language Models: An evolutionary approach to generate wrong sentences with high acceptability score, ICLR 2020. Pragmatic Evaluation of Adversarial Examples in Natural Language, ICLR 2020. "],
["compositionality-pb.html", "5.7 Compositionality", " 5.7 Compositionality Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models, ICLR 2020. Measuring Compositional Generalization: A Comprehensive Method on Realistic Data, ICLR 2020. What Can Neural Networks Reason About?, ICLR 2020. "],
["influence-function-and-knowledge-tracing.html", "5.8 Influence Function and Knowledge Tracing", " 5.8 Influence Function and Knowledge Tracing Stronger Data Poisoning Attacks Break Data Sanitization Defenses, arXiv Nov. 2018. Investigating Robustness and Interpretability of Link Prediction via Adversarial Modification, NAACL 2019. On the Accuracy of Influence Functions for Measuring Group Effects, NeurIPS 2019. Data Shapley: Equitable Valuation of Data for Machine Learning, ICML 2019. Data Cleansing for Models Trained with SGD, NeurIPS 2019. Can You Trust This Prediction? Auditing Pointwise Reliability After Learning, AISTATS 2019. Detecting Extrapolation with Local Ensembles, ICLR 2020. Multi-stage Influence Function, ICLR 2020. An Empirical and Comparative Analysis of Data Valuation with Scalable Algorithms, ICLR 2020. Data Valuation Using Reinforcement Learning, ICLR 2020. Dataset Distillation, ICLR 2020. Modelling the Influence of Data Structure on Learning in Neural Networks, ICLR 2020. Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms, VLDB 2019. Who’s responsible? Jointly quantifying the contribution of the learning algorithm and training data, arXiv Oct. 2019. "],
["statistical-techniques.html", "5.9 Statistical Techniques", " 5.9 Statistical Techniques Estimating Gradients for Discrete Random Variables By Sampling without Replacement, ICLR 2020. "],
["machine-translation.html", "5.10 Machine Translation", " 5.10 Machine Translation 5.10.1 Training Connecting The Dots Between MLE and RL for Sequence Prediction, ICLR 2020. 5.10.2 Decoding Neural Phrase-to-Phrase Machine Translation, ICLR 2020. Masked Translation Model, ICLR 2020. XL-Editor: Post-editing Sentences with XLNet, arXiv Oct. 2019. 5.10.3 Error Analysis When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis and Lexical Cohesion, ACL 2019. "],
["references.html", "References", " References "]
]
